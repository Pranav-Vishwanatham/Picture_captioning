# ðŸ–¼ï¸ Picture Captioning Using Deep Neural Networks

This project implements an end-to-end deep learning model that generates natural language captions for input images. Developed as part of the final year project for the M.Tech Integrated program at **Vellore Institute of Technology**, it uses **CNNs (InceptionV3)** for image encoding and **RNNs (LSTM)** for caption generation. The model is trained and evaluated on the **Flickr8k dataset**.

ðŸ”— **Run this notebook in Google Colab**:  
[Open in Colab](https://colab.research.google.com/drive/1eoRDfoehgl4qInMFrdtIvGh2CyTh0ImD?usp=sharing)

---

## ðŸŽ¯ Objective

- Automatically generate English captions for images
- Combine visual understanding (CNN) with language modeling (LSTM)
- Enable accessibility features like text-to-speech for visually impaired users

---

## ðŸ§  Model Overview

- **Encoder**: Pre-trained InceptionV3 CNN (without Softmax) generates a 2048-d feature vector.
- **Text Embedding**: GloVe vectors (200D) for vocabulary words.
- **Decoder**: LSTM network trained to generate captions from image and text features.
- **Output**: Caption with highest BLEU score based on training data vocabulary (1,652 words).

---

## ðŸ“ Dataset

- **Dataset**: [Flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k)
- 8,000 images total
- 6,000 for training, 2,000 for testing
- 5 captions per image

---

## ðŸ§ª Key Modules

- ðŸ–¼ï¸ Image Preprocessing (resize to 299x299)
- ðŸ§¾ Caption Cleaning and Vocabulary Mapping
- ðŸ§  Feature Extraction via InceptionV3
- ðŸ”¡ Word Embeddings via GloVe (200D)
- ðŸ” Caption Decoding via LSTM
- ðŸ”Š Text-to-Speech with `pyttsx3`
- ðŸ“ˆ Evaluation using BLEU Score

---

## ðŸ”„ Model Architecture

- Input 1: Encoded image (2048-d vector)
- Input 2: Tokenized and embedded caption
- Merge: Combine both into a dense layer
- Decoder: LSTM â†’ Dense â†’ Softmax over vocabulary

---

## ðŸ”Š Accessibility Feature

- Uses `pyttsx3` for reading out captions to assist blind or visually impaired users.

---

## ðŸ“Š Results

- BLEU Score used for evaluation
- Improved accuracy over baseline VGG16+RNN models
- Captions are more fluent and semantically aligned with image content

---

## ðŸ›  Technologies Used

- Python (Colab)
- TensorFlow / Keras
- NumPy, Pandas, Matplotlib
- InceptionV3, LSTM, GloVe
- `pyttsx3` (text-to-speech)

---

## ðŸ“Œ Future Enhancements

- Train on larger datasets (Flickr30k, MS COCO)
- Integrate with mobile or web interface
- Replace LSTM with BiLSTM or GRU
- Use ResNet/GoogleNet for improved feature encoding

---

## ðŸ“¸ Example Outputs

> Example:  
> ![Sample Output1](Results1.png)
> ![Sample Output1](Results2.png)

---


